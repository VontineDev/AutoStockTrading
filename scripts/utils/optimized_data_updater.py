"""
ë°ì´í„° ì—…ë°ì´íŠ¸ ìµœì í™” ì—”ì§„
ë°±í…ŒìŠ¤íŒ… ìµœì í™” êµ¬ì¡°ë¥¼ ì°¸ê³ í•œ ë°ì´í„° ì—…ë°ì´íŠ¸ ì „ìš© ìµœì í™” ì‹œìŠ¤í…œ

ìµœì í™” ì „ëµ:
1. ì¤‘ë³µ ì—…ë°ì´íŠ¸ ë°©ì§€ (ìºì‹±)
2. ì¦ë¶„ ì—…ë°ì´íŠ¸ (ëˆ„ë½ ë‚ ì§œë§Œ)
3. ë°°ì¹˜ API í˜¸ì¶œ ìµœì í™”
4. ë©”ëª¨ë¦¬ ê´€ë¦¬ ë° ì§„í–‰ë¥  ëª¨ë‹ˆí„°ë§
"""

import time
import logging
import sqlite3
import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional, Tuple, Callable
from pathlib import Path
from datetime import datetime, timedelta, date
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import psutil
import gc
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì •
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.append(str(PROJECT_ROOT))

# pykrx import
try:
    from pykrx import stock
except ImportError:
    print("pykrxê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install pykrx'ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
    sys.exit(1)

logger = logging.getLogger(__name__)

@dataclass
class OptimizedDataUpdateConfig:
    """ìµœì í™”ëœ ë°ì´í„° ì—…ë°ì´íŠ¸ ì„¤ì •"""
    # ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
    max_workers: int = 4
    chunk_size: int = 20
    timeout: int = 300
    
    # ìºì‹± ì„¤ì •
    enable_cache: bool = True
    cache_expiry_hours: int = 6  # ìºì‹œ ë§Œë£Œ ì‹œê°„
    check_latest_data: bool = True  # ìµœì‹  ë°ì´í„° ì²´í¬
    
    # ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì •
    batch_size: int = 30
    max_memory_usage_mb: int = 512
    adaptive_batch_size: bool = True
    
    # API ìµœì í™” ì„¤ì •
    api_delay: float = 0.3  # API í˜¸ì¶œ ê°„ê²©
    max_retries: int = 3
    retry_delay: float = 1.0
    
    # ì¦ë¶„ ì—…ë°ì´íŠ¸ ì„¤ì •
    incremental_update: bool = True
    skip_existing: bool = True
    force_update_days: int = 3  # ìµœê·¼ Nì¼ì€ ê°•ì œ ì—…ë°ì´íŠ¸
    
    # ì§„í–‰ë¥  ì½œë°±
    progress_callback: Optional[Callable] = None

class DataUpdateCacheManager:
    """ë°ì´í„° ì—…ë°ì´íŠ¸ ìºì‹± ë§¤ë‹ˆì €"""
    
def __init__(self, db_path: str, config: OptimizedDataUpdateConfig):
        self.db_path = db_path
        self.config = config
        self.memory_cache = {}  # ë©”ëª¨ë¦¬ ìºì‹œ
        self.cache_lock = threading.Lock()
        
        # ìºì‹œ í†µê³„
        self.cache_stats = {
            'hits': 0,
            'misses': 0,
            'skipped_updates': 0,
            'incremental_updates': 0,
            'full_updates': 0
        }
        
        logger.info("ë°ì´í„° ì—…ë°ì´íŠ¸ ìºì‹œ ë§¤ë‹ˆì € ì´ˆê¸°í™”")
    
def get_symbol_update_status(self, symbol: str) -> Dict[str, Any]:
        """ì¢…ëª©ì˜ ì—…ë°ì´íŠ¸ ìƒíƒœ í™•ì¸"""
        with self.cache_lock:
            # ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸
            cache_key = f"status_{symbol}"
            if cache_key in self.memory_cache:
                cached_time, status = self.memory_cache[cache_key]
                if (datetime.now() - cached_time).seconds < self.config.cache_expiry_hours * 3600:
                    self.cache_stats['hits'] += 1
                    return status
            
            # DBì—ì„œ ìƒíƒœ ì¡°íšŒ
            status = self._query_symbol_status_from_db(symbol)
            
            # ìºì‹œì— ì €ì¥
            self.memory_cache[cache_key] = (datetime.now(), status)
            self.cache_stats['misses'] += 1
            
            return status
    
def _query_symbol_status_from_db(self, symbol: str) -> Dict[str, Any]:
        """DBì—ì„œ ì¢…ëª© ìƒíƒœ ì¡°íšŒ"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                # ìµœì‹  ì—…ë°ì´íŠ¸ ë‚ ì§œ ì¡°íšŒ
                cursor = conn.execute('''
                    SELECT date, updated_at 
                    FROM stock_data 
                    WHERE symbol = ? 
                    ORDER BY date DESC 
                    LIMIT 1
                ''', (symbol,))
                result = cursor.fetchone()
                
                if result:
                    latest_date, updated_at = result
                    
                    # ë°ì´í„° ê°œìˆ˜ ì¡°íšŒ
                    cursor = conn.execute('''
                        SELECT COUNT(*) 
                        FROM stock_data 
                        WHERE symbol = ?
                    ''', (symbol,))
                    data_count = cursor.fetchone()[0]
                    
                    return {
                        'has_data': True,
                        'latest_date': latest_date,
                        'updated_at': updated_at,
                        'data_count': data_count,
                        'needs_update': self._needs_update(latest_date, updated_at)
                    }
                else:
                    return {
                        'has_data': False,
                        'latest_date': None,
                        'updated_at': None,
                        'data_count': 0,
                        'needs_update': True
                    }
        except Exception as e:
            logger.error(f"ì¢…ëª© ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨ ({symbol}): {e}")
            return {'has_data': False, 'needs_update': True}
    
def _needs_update(self, latest_date: str, updated_at: str) -> bool:
        """ì—…ë°ì´íŠ¸ í•„ìš” ì—¬ë¶€ íŒë‹¨"""
        if not latest_date or not updated_at:
            return True
        
        try:
            # ìµœì‹  ë°ì´í„° ë‚ ì§œê°€ ì˜¤ëŠ˜ ë˜ëŠ” ì–´ì œë©´ ì—…ë°ì´íŠ¸ ë¶ˆí•„ìš”
            latest_dt = datetime.strptime(latest_date, '%Y-%m-%d').date()
            today = date.today()
            yesterday = today - timedelta(days=1)
            
            # ì£¼ë§ ê³ ë ¤ (ê¸ˆìš”ì¼ ë°ì´í„°ê°€ ìˆìœ¼ë©´ OK)
            if today.weekday() in [5, 6]:  # í† , ì¼
                # ìµœê·¼ 3ì¼ ë‚´ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸ ë¶ˆí•„ìš”
                threshold_date = today - timedelta(days=3)
            else:
                # í‰ì¼ì€ ì–´ì œ ë°ì´í„°ê°€ ìˆìœ¼ë©´ OK
                threshold_date = yesterday
            
            return latest_dt < threshold_date
            
        except Exception as e:
            logger.error(f"ì—…ë°ì´íŠ¸ í•„ìš” ì—¬ë¶€ íŒë‹¨ ì‹¤íŒ¨: {e}")
            return True
    
def get_missing_date_ranges(self, symbol: str, start_date: str, end_date: str) -> List[Tuple[str, str]]:
        """ëˆ„ë½ëœ ë‚ ì§œ ë²”ìœ„ ì¡°íšŒ (ì¦ë¶„ ì—…ë°ì´íŠ¸ìš©)"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                # ê¸°ì¡´ ë°ì´í„° ë‚ ì§œë“¤ ì¡°íšŒ
                cursor = conn.execute('''
                    SELECT date 
                    FROM stock_data 
                    WHERE symbol = ? AND date BETWEEN ? AND ?
                    ORDER BY date
                ''', (symbol, start_date, end_date))
                
                existing_dates = set(row[0] for row in cursor.fetchall())
                
                # ì „ì²´ ë‚ ì§œ ë²”ìœ„ ìƒì„± (ì£¼ë§ ì œì™¸)
                start_dt = datetime.strptime(start_date, '%Y-%m-%d').date()
                end_dt = datetime.strptime(end_date, '%Y-%m-%d').date()
                
                all_dates = []
                current_date = start_dt
                while current_date <= end_dt:
                    # ì£¼ë§ ì œì™¸ (ì‹¤ì œë¡œëŠ” ê±°ë˜ì¼ ë‹¬ë ¥ì„ ì‚¬ìš©í•´ì•¼ í•˜ì§€ë§Œ ê°„ë‹¨í™”)
                    if current_date.weekday() < 5:
                        all_dates.append(current_date.strftime('%Y-%m-%d'))
                    current_date += timedelta(days=1)
                
                # ëˆ„ë½ëœ ë‚ ì§œë“¤ ì°¾ê¸°
                missing_dates = sorted(set(all_dates) - existing_dates)
                
                # ì—°ì†ëœ ë‚ ì§œë“¤ì„ ë²”ìœ„ë¡œ ê·¸ë£¹í™”
                if not missing_dates:
                    return []
                
                ranges = []
                start_range = missing_dates[0]
                end_range = missing_dates[0]
                
                for i in range(1, len(missing_dates)):
                    current_dt = datetime.strptime(missing_dates[i], '%Y-%m-%d').date()
                    prev_dt = datetime.strptime(missing_dates[i-1], '%Y-%m-%d').date()
                    
                    if (current_dt - prev_dt).days <= 3:  # ì—°ì†ìœ¼ë¡œ ê°„ì£¼ (ì£¼ë§ í¬í•¨)
                        end_range = missing_dates[i]
                    else:
                        ranges.append((start_range, end_range))
                        start_range = missing_dates[i]
                        end_range = missing_dates[i]
                
                ranges.append((start_range, end_range))
                return ranges
                
        except Exception as e:
            logger.error(f"ëˆ„ë½ ë‚ ì§œ ë²”ìœ„ ì¡°íšŒ ì‹¤íŒ¨ ({symbol}): {e}")
            return [(start_date, end_date)]  # ì‹¤íŒ¨ ì‹œ ì „ì²´ ë²”ìœ„ ë°˜í™˜
    
def should_skip_symbol(self, symbol: str) -> bool:
        """ì¢…ëª© ì—…ë°ì´íŠ¸ ê±´ë„ˆë›°ê¸° ì—¬ë¶€"""
        if not self.config.skip_existing:
            return False
        
        status = self.get_symbol_update_status(symbol)
        
        if not status['needs_update']:
            self.cache_stats['skipped_updates'] += 1
            return True
        
        return False
    
def invalidate_cache(self, symbol: str = None):
        """ìºì‹œ ë¬´íš¨í™”"""
        with self.cache_lock:
            if symbol:
                cache_key = f"status_{symbol}"
                self.memory_cache.pop(cache_key, None)
            else:
                self.memory_cache.clear()
    
def get_cache_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        total_requests = self.cache_stats['hits'] + self.cache_stats['misses']
        hit_rate = self.cache_stats['hits'] / total_requests if total_requests > 0 else 0
        
        return {
            **self.cache_stats,
            'hit_rate': hit_rate,
            'total_requests': total_requests,
            'cache_size': len(self.memory_cache)
        }

class DataUpdateBatchProcessor:
    """ë°ì´í„° ì—…ë°ì´íŠ¸ ë°°ì¹˜ í”„ë¡œì„¸ì„œ"""
    
def __init__(self, config: OptimizedDataUpdateConfig):
        self.config = config
        self.current_batch_size = config.batch_size
        self.memory_monitor = self._init_memory_monitor()
        
        # ì„±ëŠ¥ í†µê³„
        self.stats = {
            'total_batches': 0,
            'total_symbols': 0,
            'total_time': 0,
            'api_calls': 0,
            'memory_optimizations': 0,
            'batch_size_changes': 0,
            'avg_symbols_per_second': 0
        }
        
        logger.info(f"ë°°ì¹˜ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”: ë°°ì¹˜ í¬ê¸° {self.current_batch_size}")
    
def _init_memory_monitor(self):
        """ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„° ì´ˆê¸°í™”"""
        return {
            'process': psutil.Process(),
            'max_memory_mb': self.config.max_memory_usage_mb,
            'peak_memory_mb': 0,
            'gc_count': 0
        }
    
def create_optimized_batches(self, symbols: List[str]) -> List[List[str]]:
        """ìµœì í™”ëœ ë°°ì¹˜ ìƒì„±"""
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì— ë”°ë¥¸ ë°°ì¹˜ í¬ê¸° ì¡°ì •
        current_memory_mb = self.memory_monitor['process'].memory_info().rss / (1024 * 1024)
        memory_ratio = current_memory_mb / self.config.max_memory_usage_mb
        
        if memory_ratio > 0.8:
            # ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ë°°ì¹˜ í¬ê¸° ê°ì†Œ
            self.current_batch_size = max(10, int(self.current_batch_size * 0.8))
            self.stats['batch_size_changes'] += 1
            logger.info(f"ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ë°°ì¹˜ í¬ê¸° ì¡°ì •: {self.current_batch_size}")
        
        # ë°°ì¹˜ ìƒì„±
        batches = []
        for i in range(0, len(symbols), self.current_batch_size):
            batch = symbols[i:i + self.current_batch_size]
            batches.append(batch)
        
        logger.info(f"ë°°ì¹˜ ìƒì„± ì™„ë£Œ: {len(batches)}ê°œ ë°°ì¹˜, ë°°ì¹˜ í¬ê¸°: {self.current_batch_size}")
        return batches
    
def process_batch_with_monitoring(self, 
                                    batch: List[str],
                                    processor_func: Callable,
                                    **kwargs) -> Dict[str, Any]:
        """ëª¨ë‹ˆí„°ë§ì„ í¬í•¨í•œ ë°°ì¹˜ ì²˜ë¦¬"""
        batch_start_time = time.time()
        
        # ë©”ëª¨ë¦¬ ìµœì í™”
        self._optimize_memory()
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
        try:
            results = processor_func(batch, **kwargs)
            success_count = sum(1 for result in results.values() if result)
            success_rate = success_count / len(batch) if batch else 0
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            results = {symbol: False for symbol in batch}
            success_rate = 0
        
        # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
        batch_time = time.time() - batch_start_time
        self.stats['total_batches'] += 1
        self.stats['total_symbols'] += len(batch)
        self.stats['total_time'] += batch_time
        self.stats['api_calls'] += len(batch)
        
        if self.stats['total_time'] > 0:
            self.stats['avg_symbols_per_second'] = self.stats['total_symbols'] / self.stats['total_time']
        
        logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(batch)}ê°œ ì¢…ëª©, {batch_time:.2f}ì´ˆ, ì„±ê³µë¥ : {success_rate:.1%}")
        
        return {
            'results': results,
            'batch_time': batch_time,
            'success_rate': success_rate,
            'symbols_processed': len(batch)
        }
    
def _optimize_memory(self):
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        current_memory_mb = self.memory_monitor['process'].memory_info().rss / (1024 * 1024)
        
        # í”¼í¬ ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸
        if current_memory_mb > self.memory_monitor['peak_memory_mb']:
            self.memory_monitor['peak_memory_mb'] = current_memory_mb
        
        # ë©”ëª¨ë¦¬ ì„ê³„ê°’ ì´ˆê³¼ ì‹œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        if current_memory_mb > self.config.max_memory_usage_mb * 0.8:
            gc.collect()
            self.memory_monitor['gc_count'] += 1
            self.stats['memory_optimizations'] += 1
            
            new_memory_mb = self.memory_monitor['process'].memory_info().rss / (1024 * 1024)
            logger.info(f"ë©”ëª¨ë¦¬ ìµœì í™”: {current_memory_mb:.1f}MB -> {new_memory_mb:.1f}MB")
    
def get_performance_stats(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        return {
            **self.stats,
            'current_batch_size': self.current_batch_size,
            'peak_memory_mb': self.memory_monitor['peak_memory_mb'],
            'gc_count': self.memory_monitor['gc_count']
        }

class OptimizedDataUpdater:
    """ìµœì í™”ëœ ë°ì´í„° ì—…ë°ì´í„° ë©”ì¸ ì—”ì§„"""
    
def __init__(self, db_path: str, config: OptimizedDataUpdateConfig = None):
        self.db_path = db_path
        self.config = config or OptimizedDataUpdateConfig()
        
        # ìµœì í™” ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.cache_manager = DataUpdateCacheManager(db_path, self.config)
        self.batch_processor = DataUpdateBatchProcessor(self.config)
        
        # ì§„í–‰ë¥  ì¶”ì 
        self.progress_lock = threading.Lock()
        self.progress_stats = {
            'total_symbols': 0,
            'completed_symbols': 0,
            'skipped_symbols': 0,
            'failed_symbols': 0,
            'start_time': None
        }
        
        # ì „ì²´ ì„±ëŠ¥ í†µê³„
        self.performance_stats = {
            'optimization_enabled': {
                'caching': self.config.enable_cache,
                'incremental_update': self.config.incremental_update,
                'parallel_processing': True,
                'batch_optimization': True
            },
            'total_runs': 0,
            'total_time': 0,
            'total_symbols_processed': 0,
            'optimization_efficiency': 0
        }
        
        logger.info("ìµœì í™”ëœ ë°ì´í„° ì—…ë°ì´í„° ì´ˆê¸°í™” ì™„ë£Œ")
    
def update_symbols_optimized(self,
                               symbols: List[str],
                               start_date: str = None,
                               end_date: str = None,
                               force_update: bool = False) -> Dict[str, Any]:
        """ìµœì í™”ëœ ì‹¬ë³¼ ì—…ë°ì´íŠ¸"""
        
        start_time = time.time()
        logger.info(f"ìµœì í™”ëœ ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹œì‘: {len(symbols)}ê°œ ì¢…ëª©")
        
        # ì§„í–‰ë¥  ì´ˆê¸°í™”
        self.progress_stats.update({
            'total_symbols': len(symbols),
            'completed_symbols': 0,
            'skipped_symbols': 0,
            'failed_symbols': 0,
            'start_time': start_time
        })
        
        # 1. ìºì‹œ ê¸°ë°˜ í•„í„°ë§ (ê±´ë„ˆë›¸ ì¢…ëª© í™•ì¸)
        symbols_to_process = self._filter_symbols_by_cache(symbols, force_update)
        
        logger.info(f"ìºì‹œ í•„í„°ë§ ì™„ë£Œ: {len(symbols_to_process)}ê°œ ì²˜ë¦¬ ëŒ€ìƒ "
                   f"(ê±´ë„ˆë›´ ì¢…ëª©: {len(symbols) - len(symbols_to_process)}ê°œ)")
        
        # 2. ë°°ì¹˜ ìƒì„± ë° ë³‘ë ¬ ì²˜ë¦¬
        if symbols_to_process:
            batches = self.batch_processor.create_optimized_batches(symbols_to_process)
            parallel_results = self._process_batches_parallel(batches, start_date, end_date, force_update)
        else:
            parallel_results = {'results': {}, 'batch_stats': []}
        
        # 3. ì„±ëŠ¥ ë¶„ì„
        total_time = time.time() - start_time
        performance_analysis = self._analyze_performance(parallel_results, total_time, len(symbols))
        
        # 4. í†µê³„ ì—…ë°ì´íŠ¸
        self._update_performance_stats(len(symbols), total_time, performance_analysis)
        
        logger.info(f"ìµœì í™”ëœ ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ: {total_time:.2f}ì´ˆ")
        
        return {
            'results': parallel_results['results'],
            'performance_analysis': performance_analysis,
            'cache_stats': self.cache_manager.get_cache_stats(),
            'batch_stats': self.batch_processor.get_performance_stats(),
            'total_time': total_time
        }
    
def _filter_symbols_by_cache(self, symbols: List[str], force_update: bool) -> List[str]:
        """ìºì‹œ ê¸°ë°˜ ì¢…ëª© í•„í„°ë§"""
        if force_update or not self.config.enable_cache:
            return symbols
        
        symbols_to_process = []
        
        for symbol in symbols:
            if not self.cache_manager.should_skip_symbol(symbol):
                symbols_to_process.append(symbol)
            else:
                with self.progress_lock:
                    self.progress_stats['skipped_symbols'] += 1
                    
                # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ
                if self.config.progress_callback:
                    self._call_progress_callback()
        
        return symbols_to_process
    
def _process_batches_parallel(self, 
                                batches: List[List[str]], 
                                start_date: str, 
                                end_date: str,
                                force_update: bool) -> Dict[str, Any]:
        """ë°°ì¹˜ë“¤ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬"""
        
        all_results = {}
        batch_stats = []
        
def process_single_batch(batch: List[str]) -> Dict[str, Any]:
            """ë‹¨ì¼ ë°°ì¹˜ ì²˜ë¦¬"""
            return self.batch_processor.process_batch_with_monitoring(
                batch,
                self._update_batch_sequential,
                start_date=start_date,
                end_date=end_date,
                force_update=force_update
            )
        
        # ë³‘ë ¬ ë°°ì¹˜ ì²˜ë¦¬
        with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
            future_to_batch = {
                executor.submit(process_single_batch, batch): i 
                for i, batch in enumerate(batches)
            }
            
            for future in as_completed(future_to_batch, timeout=self.config.timeout):
                try:
                    batch_result = future.result()
                    all_results.update(batch_result['results'])
                    batch_stats.append(batch_result)
                    
                except Exception as e:
                    batch_idx = future_to_batch[future]
                    logger.error(f"ë°°ì¹˜ {batch_idx} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        return {
            'results': all_results,
            'batch_stats': batch_stats
        }
    
def _update_batch_sequential(self, 
                               batch: List[str],
                               start_date: str = None,
                               end_date: str = None,
                               force_update: bool = False) -> Dict[str, bool]:
        """ë°°ì¹˜ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ (API í˜¸ì¶œ ì œí•œ ê³ ë ¤)"""
        
        results = {}
        
        for symbol in batch:
            try:
                # API í˜¸ì¶œ ê°„ê²© ì¤€ìˆ˜
                time.sleep(self.config.api_delay)
                
                # ê°œë³„ ì¢…ëª© ì—…ë°ì´íŠ¸
                success = self._update_single_symbol_optimized(symbol, start_date, end_date, force_update)
                results[symbol] = success
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                with self.progress_lock:
                    if success:
                        self.progress_stats['completed_symbols'] += 1
                    else:
                        self.progress_stats['failed_symbols'] += 1
                    
                    # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ
                    if self.config.progress_callback:
                        self._call_progress_callback()
                
            except Exception as e:
                logger.error(f"ì¢…ëª© {symbol} ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                results[symbol] = False
                
                with self.progress_lock:
                    self.progress_stats['failed_symbols'] += 1
        
        return results
    
def _update_single_symbol_optimized(self, 
                                      symbol: str, 
                                      start_date: str = None, 
                                      end_date: str = None,
                                      force_update: bool = False) -> bool:
        """ìµœì í™”ëœ ë‹¨ì¼ ì¢…ëª© ì—…ë°ì´íŠ¸"""
        
        try:
            # ì¦ë¶„ ì—…ë°ì´íŠ¸ ì‚¬ìš© ì—¬ë¶€ í™•ì¸
            if self.config.incremental_update and not force_update:
                missing_ranges = self.cache_manager.get_missing_date_ranges(symbol, start_date, end_date)
                
                if not missing_ranges:
                    logger.debug(f"ì¢…ëª© {symbol}: ì—…ë°ì´íŠ¸ ë¶ˆí•„ìš”")
                    return True
                
                # ëˆ„ë½ëœ ë²”ìœ„ë§Œ ì—…ë°ì´íŠ¸
                for range_start, range_end in missing_ranges:
                    if not self._fetch_and_save_data(symbol, range_start, range_end):
                        return False
                
                self.cache_manager.cache_stats['incremental_updates'] += 1
            else:
                # ì „ì²´ ì—…ë°ì´íŠ¸
                if not self._fetch_and_save_data(symbol, start_date, end_date):
                    return False
                
                self.cache_manager.cache_stats['full_updates'] += 1
            
            # ìºì‹œ ë¬´íš¨í™”
            self.cache_manager.invalidate_cache(symbol)
            return True
            
        except Exception as e:
            logger.error(f"ì¢…ëª© {symbol} ìµœì í™” ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            return False
    
def _fetch_and_save_data(self, symbol: str, start_date: str, end_date: str) -> bool:
        """ë°ì´í„° ì¡°íšŒ ë° ì €ì¥"""
        try:
            # pykrxë¡œ ë°ì´í„° ì¡°íšŒ
            df = stock.get_market_ohlcv_by_date(start_date, end_date, symbol)
            
            if df.empty:
                logger.warning(f"ì¢…ëª© {symbol}: ë°ì´í„° ì—†ìŒ ({start_date}~{end_date})")
                return False
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            self._save_data_to_db(symbol, df)
            return True
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ì¡°íšŒ/ì €ì¥ ì‹¤íŒ¨ ({symbol}): {e}")
            return False
    
def _save_data_to_db(self, symbol: str, df: pd.DataFrame):
        """ë°ì´í„°ë² ì´ìŠ¤ì— ë°ì´í„° ì €ì¥"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                # ë°ì´í„° ë³€í™˜
                df_to_save = df.copy()
                df_to_save['symbol'] = symbol
                df_to_save['date'] = df_to_save.index.strftime('%Y-%m-%d')
                df_to_save = df_to_save.reset_index(drop=True)
                
                # ì»¬ëŸ¼ëª… ë§¤í•‘ (pykrx -> DB)
                column_mapping = {
                    'ì‹œê°€': 'open',
                    'ê³ ê°€': 'high', 
                    'ì €ê°€': 'low',
                    'ì¢…ê°€': 'close',
                    'ê±°ë˜ëŸ‰': 'volume',
                    'ê±°ë˜ëŒ€ê¸ˆ': 'amount'
                }
                df_to_save = df_to_save.rename(columns=column_mapping)
                
                # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
                required_columns = ['symbol', 'date', 'open', 'high', 'low', 'close', 'volume']
                if 'amount' in df_to_save.columns:
                    required_columns.append('amount')
                
                df_final = df_to_save[required_columns]
                
                # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (REPLACE ì‚¬ìš©ìœ¼ë¡œ ì¤‘ë³µ ì²˜ë¦¬)
                df_final.to_sql('stock_data', conn, if_exists='append', index=False, method='multi')
                
        except Exception as e:
            logger.error(f"DB ì €ì¥ ì‹¤íŒ¨ ({symbol}): {e}")
            raise
    
def _call_progress_callback(self):
        """ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ"""
        total = self.progress_stats['total_symbols']
        completed = (self.progress_stats['completed_symbols'] + 
                    self.progress_stats['skipped_symbols'] + 
                    self.progress_stats['failed_symbols'])
        
        progress = completed / total if total > 0 else 0
        
        self.config.progress_callback(
            progress=progress,
            completed=completed,
            total=total,
            start_time=self.progress_stats['start_time']
        )
    
def _analyze_performance(self, parallel_results: Dict, total_time: float, total_symbols: int) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë¶„ì„"""
        
        cache_stats = self.cache_manager.get_cache_stats()
        batch_stats = self.batch_processor.get_performance_stats()
        
        successful_updates = sum(1 for result in parallel_results['results'].values() if result)
        symbols_per_second = total_symbols / total_time if total_time > 0 else 0
        
        # ìµœì í™” íš¨ìœ¨ì„± ê³„ì‚°
        cache_efficiency = cache_stats['hit_rate'] if cache_stats['total_requests'] > 0 else 0
        skip_efficiency = cache_stats['skipped_updates'] / total_symbols if total_symbols > 0 else 0
        incremental_efficiency = cache_stats['incremental_updates'] / max(1, cache_stats['incremental_updates'] + cache_stats['full_updates'])
        
        overall_efficiency = (cache_efficiency + skip_efficiency + incremental_efficiency) / 3
        
        return {
            'total_time': total_time,
            'total_symbols': total_symbols,
            'successful_updates': successful_updates,
            'success_rate': successful_updates / total_symbols if total_symbols > 0 else 0,
            'symbols_per_second': symbols_per_second,
            'cache_efficiency': cache_efficiency,
            'skip_efficiency': skip_efficiency,
            'incremental_efficiency': incremental_efficiency,
            'overall_efficiency': overall_efficiency,
            'memory_peak_mb': batch_stats['peak_memory_mb'],
            'api_calls': batch_stats['api_calls']
        }
    
def _update_performance_stats(self, symbols_count: int, total_time: float, performance_analysis: Dict):
        """ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸"""
        self.performance_stats['total_runs'] += 1
        self.performance_stats['total_time'] += total_time
        self.performance_stats['total_symbols_processed'] += symbols_count
        self.performance_stats['optimization_efficiency'] = performance_analysis['overall_efficiency']
    
def get_optimization_stats(self) -> Dict[str, Any]:
        """ìµœì í™” í†µê³„ ë°˜í™˜"""
        return {
            'performance_stats': self.performance_stats,
            'cache_stats': self.cache_manager.get_cache_stats(),
            'batch_stats': self.batch_processor.get_performance_stats(),
            'progress_stats': self.progress_stats
        }
    
def clear_all_caches(self):
        """ëª¨ë“  ìºì‹œ í´ë¦¬ì–´"""
        self.cache_manager.invalidate_cache()
        logger.info("ëª¨ë“  ìºì‹œê°€ í´ë¦¬ì–´ë˜ì—ˆìŠµë‹ˆë‹¤")

def create_optimized_data_updater(db_path: str,
                                max_workers: int = 4,
                                enable_cache: bool = True,
                                batch_size: int = 30,
                                progress_callback: Callable = None) -> OptimizedDataUpdater:
    """ìµœì í™”ëœ ë°ì´í„° ì—…ë°ì´í„° ìƒì„±"""
    
    config = OptimizedDataUpdateConfig(
        max_workers=max_workers,
        enable_cache=enable_cache,
        batch_size=batch_size,
        progress_callback=progress_callback
    )
    
    return OptimizedDataUpdater(db_path, config)

def progress_callback_with_eta(progress: float, completed: int, total: int, start_time: float = None):
    """ETA í¬í•¨ ì§„í–‰ë¥  ì½œë°±"""
    if start_time:
        elapsed_time = time.time() - start_time
        if progress > 0:
            eta = elapsed_time * (1 - progress) / progress
            eta_str = f", ETA: {eta:.1f}ì´ˆ"
        else:
            eta_str = ""
    else:
        eta_str = ""
    
    print(f"\rğŸ“Š ì§„í–‰ë¥ : {progress:.1%} ({completed}/{total}){eta_str}", end="", flush=True) 